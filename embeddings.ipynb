{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95f841a-63c9-41d4-aea1-496b3d2024dd",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "<tr>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<font size=\"2\">\n",
    "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
    "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
    "</font>\n",
    "</td>\n",
    "<td style=\"vertical-align:middle; text-align:left;\">\n",
    "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f25706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (2.10.0)\n",
      "Requirement already satisfied: tiktoken in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (0.12.0)\n",
      "Requirement already satisfied: filelock in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (3.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (82.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (2026.2.0)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch) (1.3.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from tiktoken) (2026.2.19)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa40e3-5109-433f-9153-f5770531fe94",
   "metadata": {},
   "source": [
    "# Chapter 2: Working with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5d2c0-cba8-404e-9bf3-71a218cae3cf",
   "metadata": {},
   "source": [
    "Packages that are being used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.10.0\n",
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42fbfd-e3c2-43c2-bc12-f5f870a0b10a",
   "metadata": {},
   "source": [
    "- This chapter covers data preparation and sampling to get input data \"ready\" for the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b2922-594d-4ff9-bd82-04f1ebdf41f5",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/01.webp?timestamp=1\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417139b-2357-44d2-bd67-23f5d7f52ae7",
   "metadata": {},
   "source": [
    "## 2.1 Understanding word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6816ae-e927-43a9-b4dd-e47a9b0e1cf6",
   "metadata": {},
   "source": [
    "- No code in this section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69dab7-a433-427a-9e5b-b981062d6296",
   "metadata": {},
   "source": [
    "- There are many forms of embeddings; we focus on text embeddings in this book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08d16f-f237-4166-bf89-0e9fe703e7b4",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/02.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c4faf-b93a-4616-9276-7a4aa4b5e9ba",
   "metadata": {},
   "source": [
    "- LLMs work with embeddings in high-dimensional spaces (i.e., thousands of dimensions)\n",
    "- Since we can't visualize such high-dimensional spaces (we humans think in 1, 2, or 3 dimensions), the figure below illustrates a 2-dimensional embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b80160-1f10-4aad-a85e-9c79444de9e6",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/03.webp\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbb984-8d23-40c5-bbfa-c3c379e7eec3",
   "metadata": {},
   "source": [
    "## 2.2 Tokenizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c90731-7dc9-4cd3-8c4a-488e33b48e80",
   "metadata": {},
   "source": [
    "- In this section, we tokenize text, which means breaking text into smaller units, such as individual words and punctuation characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09872fdb-9d4e-40c4-949d-52a01a43ec4b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/04.webp\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cceaa18-833d-46b6-b211-b20c53902805",
   "metadata": {},
   "source": [
    "- Load raw text we want to work with\n",
    "- [The Verdict by Edith Wharton](https://en.wikisource.org/wiki/The_Verdict) is a public domain short story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f9d9b1-6d32-485a-825a-a95392a86d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport urllib.request\\n\\nif not os.path.exists(\"the-verdict.txt\"):\\n    url = (\"https://raw.githubusercontent.com/rasbt/\"\\n           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\\n           \"the-verdict.txt\")\\n    file_path = \"the-verdict.txt\"\\n    urllib.request.urlretrieve(url, file_path)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "        \"the-verdict.txt\"\n",
    "    )\n",
    "    file_path = \"the-verdict.txt\"\n",
    "\n",
    "    response = requests.get(url, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "\n",
    "# The book originally used the following code below\n",
    "# However, urllib uses older protocol settings that\n",
    "# can cause problems for some readers using a VPN.\n",
    "# The `requests` version above is more robust\n",
    "# in that regard.\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56488f2c-a2b8-49f1-aaeb-461faad08dce",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Troubleshooting SSL certificate errors\n",
    "\n",
    "- Some readers reported seeing ssl.SSLCertVerificationError: `SSL: CERTIFICATE_VERIFY_FAILED` when running `urllib.request.urlretrieve` in VSCode or Jupyter. \n",
    "- This usually means Python's certificate bundle is outdated.\n",
    "\n",
    "\n",
    "**Fixes**\n",
    "\n",
    "- Use Python â‰¥ 3.9; you can check your Python version by executing the following code:\n",
    "```python\n",
    "import sys\n",
    "print(sys.__version__)\n",
    "```\n",
    "- Upgrade the cert bundle:\n",
    "  - pip: `pip install --upgrade certifi`\n",
    "  - uv: `uv pip install --upgrade certifi`\n",
    "- Restart the Jupyter kernel after upgrading.\n",
    "- If you still encounter an `ssl.SSLCertVerificationError` when executing the previous code cell, please see the discussion at [more information here on GitHub](https://github.com/rasbt/LLMs-from-scratch/pull/403)\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a769e87-470a-48b9-8bdb-12841b416198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b971a46-ac03-4368-88ae-3f20279e8f4e",
   "metadata": {},
   "source": [
    "- The goal is to tokenize and embed this text for an LLM\n",
    "- Let's develop a simple tokenizer based on some simple sample text that we can then later apply to the text above\n",
    "- The following regular expression will split on whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737dd5b0-9dbb-4a97-9ae4-3482c8c04be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c40c18-a9d5-4703-bf71-8261dbcc5ee3",
   "metadata": {},
   "source": [
    "- We don't only want to split on whitespaces but also commas and periods, so let's modify the regular expression to do that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea02489d-01f9-4247-b7dd-a0d63f62ef07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461d0c86-e3af-4f87-8fae-594a9ca9b6ad",
   "metadata": {},
   "source": [
    "- As we can see, this creates empty strings, let's remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8a6fb7-2e62-4a12-ad06-ccb04f25fed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "# Strip whitespace from each item and then filter out any empty strings.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e8694-181e-496f-895d-7cb7d92b5562",
   "metadata": {},
   "source": [
    "- This looks pretty good, but let's also handle other types of punctuation, such as periods, question marks, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3a9467-04b4-49d9-96c5-b8042bcf8374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbea70b-c030-45d9-b09d-4318164c0bb4",
   "metadata": {},
   "source": [
    "- This is pretty good, and we are now ready to apply this tokenization to the raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe9330-b587-4262-be9f-497a84ec0e8a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/05.webp\" width=\"350px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c567caa-8ff5-49a8-a5cc-d365b0a78a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a19e1a-5105-4ddb-812a-b7d3117eab95",
   "metadata": {},
   "source": [
    "- Let's calculate the total number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35db7b5e-510b-4c45-995f-f5ad64a8e19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623bc689",
   "metadata": {},
   "source": [
    "## Why it is matter\n",
    "\n",
    "\n",
    "All of the above is important, because when we want to tokenize a set of words, text or pages, we don't want the model to be affected by characters that can be misinterpreted, so regular expressions can prevent this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231",
   "metadata": {},
   "source": [
    "## 2.3 Converting tokens into token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5204973-f414-4c0d-87b0-cfec1f06e6ff",
   "metadata": {},
   "source": [
    "- Next, we convert the text tokens into token IDs that we can process via embedding layers later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b041d-f739-43b8-bd81-0443ae3a7f8d",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/06.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5973794-7002-4202-8b12-0900cd779720",
   "metadata": {},
   "source": [
    "- From these tokens, we can now build a vocabulary that consists of all the unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fdf0533-5ab6-42a5-83fa-a3b045de6396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77d00d96-881f-4691-bb03-84fec2a75a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd1f81-3a8f-4dd9-9dd6-e75f32dacbe3",
   "metadata": {},
   "source": [
    "- Below are the first 50 entries in this vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1c5de4a-aa4e-4aec-b532-10bb364039d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dc314-351b-476a-9459-0ec9ddc29b19",
   "metadata": {},
   "source": [
    "- Below, we illustrate the tokenization of a short sample text using a small vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67407a9f-0202-4e7c-9ed7-1b3154191ebc",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/07.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e569647-2589-4c9d-9a5c-aef1c88a0a9a",
   "metadata": {},
   "source": [
    "- Putting it now all together into a tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f531bf46-7c25-4ef8-bff8-0d27518676d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7",
   "metadata": {},
   "source": [
    "- The `encode` function turns text into token IDs\n",
    "- The `decode` function turns token IDs back into text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21d347-ec03-4823-b3d4-9d686e495617",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/08.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2950a94-6b0d-474e-8ed0-66d0c3c1a95c",
   "metadata": {},
   "source": [
    "- We can use the tokenizer to encode (that is, tokenize) texts into integers\n",
    "- These integers can then be embedded (later) as input of/for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "647364ec-7995-4654-9b4a-7607ccf5f1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201706e-a487-4b60-b99d-5765865f29a0",
   "metadata": {},
   "source": [
    "- We can decode the integers back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01d8c8fb-432d-4a49-b332-99f23b233746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54f6aa8b-9827-412e-9035-e827296ab0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d738bc0",
   "metadata": {},
   "source": [
    "## Why it is Matter Encoding\n",
    "\n",
    "Performing the entire token generation process, for coding purposes, will allow us to have and order or know in alphabetical order how it has been previously organized, since later on when the entire embedding process begins it will necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b821ef8-4d53-43b6-a2b2-aef808c343c7",
   "metadata": {},
   "source": [
    "## 2.4 Adding special context tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d6d15-a3e2-44e0-b384-bb37f17cf443",
   "metadata": {},
   "source": [
    "- It's useful to add some \"special\" tokens for unknown words and to denote the end of a text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7fc96c-e1fd-44fb-b7f5-229d7c7922a4",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/09.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d709d57-2486-4152-b7f9-d3e4bd8634cd",
   "metadata": {},
   "source": [
    "- Some tokenizers use special tokens to help the LLM with additional context\n",
    "- Some of these special tokens are\n",
    "  - `[BOS]` (beginning of sequence) marks the beginning of text\n",
    "  - `[EOS]` (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia articles or two different books, and so on)\n",
    "  - `[PAD]` (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)\n",
    "- `[UNK]` to represent words that are not included in the vocabulary\n",
    "\n",
    "- Note that GPT-2 does not need any of these tokens mentioned above but only uses an `<|endoftext|>` token to reduce complexity\n",
    "- The `<|endoftext|>` is analogous to the `[EOS]` token mentioned above\n",
    "- GPT also uses the `<|endoftext|>` for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyways, so it does not matter what these tokens are)\n",
    "- GPT-2 does not use an `<UNK>` token for out-of-vocabulary words; instead, GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units which we will discuss in a later section\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336b43b-7173-49e7-bd80-527ad4efb271",
   "metadata": {},
   "source": [
    "- We use the `<|endoftext|>` tokens between two independent sources of text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52442951-752c-4855-9752-b121a17fef55",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/10.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661a397-da06-4a86-ac27-072dbe7cb172",
   "metadata": {},
   "source": [
    "- Let's see what happens if we tokenize the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5767eff-440c-4de1-9289-f789349d6b85",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = SimpleTokenizerV1(vocab)\n\u001b[32m      3\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea. Is this-- a test?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m      9\u001b[39m preprocessed = [\n\u001b[32m     10\u001b[39m     item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()\n\u001b[32m     11\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"Hello, do you like tea. Is this-- a test?\"\n",
    "\n",
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53ee0c-fe2b-4cd8-a946-5471f7651acf",
   "metadata": {},
   "source": [
    "- The above produces an error because the word \"Hello\" is not contained in the vocabulary\n",
    "- To deal with such cases, we can add special tokens like `\"<|unk|>\"` to the vocabulary to represent unknown words\n",
    "- Since we are already extending the vocabulary, let's add another token called `\"<|endoftext|>\"` which is used in GPT-2 training to denote the end of a text (and it's also used between concatenated text, like if our training datasets consists of multiple articles, books, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce9df29c-6c5b-43f1-8c1a-c7f7b79db78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57c3143b-e860-4d3b-a22a-de22b547a6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50e51bb1-ae05-4aa8-a9ff-455b65ed1959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1daa2b0-6e75-412b-ab53-1f6fb7b4d453",
   "metadata": {},
   "source": [
    "- We also need to adjust the tokenizer accordingly so that it knows when and how to use the new `<unk>` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "948861c5-3f30-4712-a234-725f20d26f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa728dd1-9d35-4ac7-938f-d411d73083f6",
   "metadata": {},
   "source": [
    "Let's try to tokenize text with the modified tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4133c502-18ac-4412-9f43-01caf4efa3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ed395fe-dc1b-4ed2-b85b-457cc35aab60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "059367f9-7a60-4c0d-8a00-7c4c766d0ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa26723",
   "metadata": {},
   "source": [
    "## Why it is matters\n",
    "\n",
    "If it's the first text or one of these, we need the model to have an understanding of each word, but proceed to encoding. However, if there are words that are not in the vocabulary, we can prevent this through tags, facilitating learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ba34b-170f-4e71-939b-77aabb776f14",
   "metadata": {},
   "source": [
    "## 2.5 BytePair encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309494c-79cf-4a2d-bc28-a94d602f050e",
   "metadata": {},
   "source": [
    "- GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
    "- it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
    "- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
    "- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "- In this chapter, we are using the BPE tokenizer from OpenAI's open-source [tiktoken](https://github.com/openai/tiktoken) library, which implements its core algorithms in Rust to improve computational performance\n",
    "- I created a notebook in the [./bytepair_encoder](../02_bonus_bytepair-encoder) that compares these two implementations side-by-side (tiktoken was about 5x faster on the sample text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ede1d41f-934b-4bf4-8184-54394a257a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from tiktoken) (2026.2.19)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tulio/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48967a77-7d17-42bf-9e92-fc619d63a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ff2cd85-7cfb-4325-b390-219938589428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d26a48bb-f82e-41a8-a955-a1c9cf9d50ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2e7b4-6a22-42aa-8e4d-901f06378d4a",
   "metadata": {},
   "source": [
    "- BPE tokenizers break down unknown words into subwords and individual characters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082d41f-33d7-4827-97d8-993d5a84bb3c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/11.webp\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c86edef",
   "metadata": {},
   "source": [
    "## Why it is Matter\n",
    "\n",
    "This method used by the gpt-2 model seeks to solve or prevent when we have a word that is not in our vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd7c0d-70f8-4386-a114-907e96c950b0",
   "metadata": {},
   "source": [
    "## 2.6 Data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d9826-6384-462e-aa8a-a7c73cd6aad0",
   "metadata": {},
   "source": [
    "- We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb44f4-0c43-4a6a-9c2f-9cf31452354c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "848d5ade-fd1f-46c3-9e31-1426e315c71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd0657-5543-43ca-8011-2ae6bd0a5810",
   "metadata": {},
   "source": [
    "- For each text chunk, we want the inputs and targets\n",
    "- Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e84424a7-646d-45b6-99e3-80d15fb761f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfbff852-a92f-48c8-a46d-143a0f109f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815014ef-62f7-4476-a6ad-66e20e42b7c3",
   "metadata": {},
   "source": [
    "- One by one, the prediction would look like as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d97b031e-ed55-409d-95f2-aeb38c6fe366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f57bd746-dcbf-4433-8e24-ee213a8c34a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d2dd9-fc20-4927-8d3d-1466cf41aae1",
   "metadata": {},
   "source": [
    "- We will take care of the next-word prediction in a later chapter after we covered the attention mechanism\n",
    "- For now, we implement a simple data loader that iterates over the input dataset and returns the inputs and targets shifted by one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1b47a-f646-49d1-bc70-fddf2c840796",
   "metadata": {},
   "source": [
    "- Install and import PyTorch (see Appendix A for installation tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1770134-e7f3-4725-a679-e04c3be48cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c",
   "metadata": {},
   "source": [
    "- We use a sliding window approach, changing the position by +1:\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac652d-7b38-4843-9fbd-494cdc8ec12c",
   "metadata": {},
   "source": [
    "- Create dataset and dataloader that extract chunks from the input text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74b41073-4c9f-46e2-a1bd-d38e4122b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5eb30ebe-97b3-43c5-9ff1-a97d621b3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd68ef-59f7-45ff-ba44-e311c899ddcd",
   "metadata": {},
   "source": [
    "- Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df31d96c-6bfd-4564-a956-6192242d7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9226d00c-ad9a-4949-a6e4-9afccfc7214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10deb4bc-4de1-4d20-921e-4b1c7a0e1a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006212f-de45-468d-bdee-5806216d1679",
   "metadata": {},
   "source": [
    "- An example using stride equal to the context length (here: 4) as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/14.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae6d45-f26e-4b83-9c7b-cff55ffa7d16",
   "metadata": {},
   "source": [
    "- We can also create batched outputs\n",
    "- Note that we increase the stride here so that we don't have overlaps between the batches, since more overlap could lead to increased overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1916e7a6-f03d-4f09-91a6-d0bdbac5a58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3178ce",
   "metadata": {},
   "source": [
    "## Why it is Matter\n",
    "\n",
    "The Sliding Window technique will allow our model to predict the next word, since we have previously trained it using this feature where we use a stride to avoid problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd2fcda-2fda-4aa8-8bc8-de1e496f9db1",
   "metadata": {},
   "source": [
    "## 2.7 Creating token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a301068-6ab2-44ff-a915-1ba11688274f",
   "metadata": {},
   "source": [
    "- The data is already almost ready for an LLM\n",
    "- But lastly let us embed the tokens in a continuous vector representation using an embedding layer\n",
    "- Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85089aa-8671-4e5f-a2b3-ef252004ee4c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/15.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e014ca-1fc5-4b90-b6fa-c2097bb92c0b",
   "metadata": {},
   "source": [
    "- Suppose we have the following four input examples with input ids 2, 3, 5, and 1 (after tokenization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15a6304c-9474-4470-b85d-3991a49fa653",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da6344-2c71-4837-858d-dd120005ba05",
   "metadata": {},
   "source": [
    "- For the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93cb2cee-9aa6-4bb8-8977-c65661d16eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff241f6-78eb-4e4a-a55f-5b2b6196d5b0",
   "metadata": {},
   "source": [
    "- This would result in a 6x3 weight matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a686eb61-e737-4351-8f1c-222913d47468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcf4f5-0801-4eb4-bb90-acce87935ac7",
   "metadata": {},
   "source": [
    "- For those who are familiar with one-hot encoding, the embedding layer approach above is essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication in a fully-connected layer, which is described in the supplementary code in [./embedding_vs_matmul](../03_bonus_embedding-vs-matmul)\n",
    "- Because the embedding layer is just a more efficient implementation that is equivalent to the one-hot encoding and matrix-multiplication approach it can be seen as a neural network layer that can be optimized via backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d58c3-83c0-4205-aca2-9c48b19fd4a7",
   "metadata": {},
   "source": [
    "- To convert a token with id 3 into a 3-dimensional vector, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e43600ba-f287-4746-8ddf-d0f71a9023ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bbf625-4f36-491d-87b4-3969efb784b0",
   "metadata": {},
   "source": [
    "- Note that the above is the 4th row in the `embedding_layer` weight matrix\n",
    "- To embed all four `input_ids` values above, we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50280ead-0363-44c8-8c35-bb885d92c8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be97ced4-bd13-42b7-866a-4d699a17e155",
   "metadata": {},
   "source": [
    "- An embedding layer is essentially a look-up operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c2741-bf1b-4c60-b7fd-61409d556646",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/16.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08218d9f-aa1a-4afb-a105-72ff96a54e73",
   "metadata": {},
   "source": [
    "- **You may be interested in the bonus content comparing embedding layers with regular linear layers: [../03_bonus_embedding-vs-matmul](../03_bonus_embedding-vs-matmul)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393d270-b950-4bc8-99ea-97d74f2ea0f6",
   "metadata": {},
   "source": [
    "## 2.8 Encoding word positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24940068-1099-4698-bdc0-e798515e2902",
   "metadata": {},
   "source": [
    "- Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b14a2-f3f3-490e-b513-f262dbcf94fa",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/17.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7d7fe-38a5-46e6-8db6-b688887b0430",
   "metadata": {},
   "source": [
    "- Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de37db-d54d-45c4-ab3e-88c0783ad2e4",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f187f87-c1f8-4c2e-8050-350bbb972f55",
   "metadata": {},
   "source": [
    "- The BytePair encoder has a vocabulary size of 50,257:\n",
    "- Suppose we want to encode the input tokens into a 256-dimensional vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b9e344d-03a6-4f2c-b723-67b6a20c5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2654722-24e4-4b0d-a43c-436a461eb70b",
   "metadata": {},
   "source": [
    "- If we sample data from the dataloader, we embed the tokens in each batch into a 256-dimensional vector\n",
    "- If we have a batch size of 8 with 4 tokens each, this results in a 8 x 4 x 256 tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad56a263-3d2e-4d91-98bf-d0b68d3c7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 6\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84416b60-3707-4370-bcbc-da0b62f2b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464,  1807,  3619],\n",
      "        [  402,   271, 10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257,   922,  5891],\n",
      "        [ 1576,   438,   568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502,   284,  3285],\n",
      "        [  326,    11,   287,   262,  6001,   286],\n",
      "        [  465, 13476,    11,   339,   550,  5710],\n",
      "        [  465, 12036,    11,  6405,   257,  5527]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 6])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7766ec38-30d0-4128-8c31-f49f063c43d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "# print(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ae164-6f19-4e32-b9e5-76950fcf1c9f",
   "metadata": {},
   "source": [
    "- GPT-2 uses absolute position embeddings, so we just create another embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cc048e20-7ac8-417e-81f5-8fe6f9a4fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "# uncomment & execute the following line to see how the embedding layer weights look like\n",
    "# print(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c369a1e7-d566-4b53-b398-d6adafb44105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "# print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e9d9f-2935-461a-9518-6d1386b976d6",
   "metadata": {},
   "source": [
    "- To create the input embeddings used in an LLM, we simply add the token and the positional embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b22fab89-526e-43c8-9035-5b7018e34288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "# print(input_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbda581-6f9b-476f-8ea7-d244e6a4eaec",
   "metadata": {},
   "source": [
    "- In the initial phase of the input processing workflow, the input text is segmented into separate tokens\n",
    "- Following this segmentation, these tokens are transformed into token IDs based on a predefined vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb0f7e-460d-44db-b366-096adcd84fff",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/19.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63230f2e-258f-4497-9e2e-8deee4530364",
   "metadata": {},
   "source": [
    "# Summary and takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3293a6-45a5-47cd-aa00-b23e3ca0a73f",
   "metadata": {},
   "source": [
    "See the [./dataloader.ipynb](./dataloader.ipynb) code notebook, which is a concise version of the data loader that we implemented in this chapter and will need for training the GPT model in upcoming chapters.\n",
    "\n",
    "See [./exercise-solutions.ipynb](./exercise-solutions.ipynb) for the exercise solutions.\n",
    "\n",
    "See the [Byte Pair Encoding (BPE) Tokenizer From Scratch](../02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb) notebook if you are interested in learning how the GPT-2 tokenizer can be implemented and trained from scratch."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "d1564698",
   "metadata": {},
   "source": [
    "## Why do embeddings encode meaning, and how are they related to NN concepts?\n",
    "\n",
    "It relates to the concept of neural network, due to the fully connected embedding layer that seeks to optimize via backpropagation through not encoding\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "By changing max_length, we are also changing the stride, obtaining a tensor of size 8x6 as seen in the image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
